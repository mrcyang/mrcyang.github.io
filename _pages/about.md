---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

# üôç‚Äç‚ôÇÔ∏è Short Bio
I am currently a Ph.D candidate in the Faculty of Education (Academic Unit of Mathematics, Science, and Technology) at The University of Hong Kong (HKU)üá≠üá∞. I am co-supervised by <a href="https://web.edu.hku.hk/faculty-academics/gwchen"> Prof. CHEN, Gaowei </a> and <a href="http://www.eie.polyu.edu.hk/~mwmak/"> Prof. FENG, Shihui </a>. Before joining HKU, I recived my Master's Degree in Computer Technology from Peking University (PKU), supervised by <a href="https://www.ss.pku.edu.cn/teacherteam/teacherlist/1640-%E4%BF%9E%E6%95%AC%E6%9D%BE.html"> Prof. YU, Jingsong </a>.

My research interests include learning sciences and technology, data visualization, classroom dialogue, and teacher professional development. My current research focuses on creating systems for personalized and collaborative learning/teaching using visual learning analytics, and other emerging techniques (e.g., AI in education). 




# üìñ Educations
- 2021.09 - present, Ph.D. candidate in Educational Technology, The University of Hong Kong, Hong Kong SAR. 
- 2014.09 - 2017.06, M.E. in Computer Technology, Peking University, Beijing. 
- 2010-09 - 2014.06, B.A. in English Language and Literature, University of Emergency Management, Hebei. 

# üíª Internship Experience
- 2023.04 - 2024.04, Research Asistant, supervised by <a href="https://sds.cuhk.edu.cn/en/teacher/498">Prof. Haizhou Li</a> and <a href="https://wsstriving.github.io">Shuai Wang</a>, The Chinese University of Hong Kong (Shenzhen). [[Project Demo]](../videos/Junjie_xinyi.mp4)
- 2022.06 - 2022.12, supervised by <a href='https://scholar.google.com/citations?user=BcWMSE4AAAAJ&hl=zh-CN'>Dr. Shiliang Zhang</a>, Alibaba DAMO Academy, Hangzhou.
- 2021.11 - 2022.01, <a href='https://e.huawei.com/cn/products/enterprise-collaboration/ideahub'>ICT</a>, Huawei, Dongguan. 


# üìù Publications

## 2024 
- MoMuSE: Momentum Multi-modal target Speaker Extraction for scenarios with impaired visual cues [[demo]](../demo_page/MoMuSE/index.html) 
- Zhang Ke, **<u>Li Junjie</u>**, et al. Multi-Level Speaker Representation for Target Speaker Extraction(submitted to ICASSP2025)
- **<u>Li Junjie</u>**, Zhang Ke, Wang Shuai, Li Haizhou, Mak Man-Wai, Lee Kong Aik "On the effectiveness of enrollment speech augmentation for Target Speaker Extraction." arXiv preprint arXiv:2409.09589 (2024).(accepted by SLT2024) [![](https://img.shields.io/github/stars/wenet-e2e/wesep?style=social&label=Code+Stars)](https://github.com/wenet-e2e/wesep)
- Wang Jiahe, Wang Shuai,  **<u>Li Junjie</u>**, et al. ENHANCING SPEAKER EXTRACTION THROUGH RECTIFYING TARGET CONFUSION (accepted by SLT2024) [![](https://img.shields.io/github/stars/wenet-e2e/wesep?style=social&label=Code+Stars)](https://github.com/wenet-e2e/wesep)
- Wang Shuai, Zhang Ke, Lin Shaoxiong, **<u>Li Junjie</u>**, et al. (2024) WeSep: A Scalable and Flexible Toolkit Towards Generalizable Target Speaker Extraction. Proc. Interspeech 2024, 4273-4277, doi: 10.21437/Interspeech.2024-1840   [![](https://img.shields.io/github/stars/wenet-e2e/wesep?style=social&label=Code+Stars)](https://github.com/wenet-e2e/wesep)
- Yang Hongli, Chen Xinyi, **<u>Li Junjie</u>**, et al. Listen to the Speaker in Your Gaze," 2024 IEEE International Conference on Cybernetics and Intelligent Systems (CIS) and IEEE International Conference on Robotics, Automation and Mechatronics (RAM), Hangzhou, China, 2024, pp. 380-385, doi: 10.1109/CIS-RAM61939.2024.10672879.
- Tao Ruijie, Qian Xinyuan,Jiang Yidi, **<u>Li Junjie</u>**, Wang Jiadong, Li Haizhou Subtracting the unseen sounds: Reverse auditory
selective hearing in audio-visual target speaker extraction. (Submitted to TASLP). 
- **<u>Li Junjie</u>**, Tao Ruijie, et al. ‚ÄúAudio-Visual Active Speaker Extraction for Sparsely Overlapped Multi-talker
Speech,‚Äù ICASSP 2024, pp. 10666-10670, doi: 10.1109/ICASSP48485.2024.10448398.   [![](https://img.shields.io/github/stars/mrjunjieli/ActiveExtract?style=social&label=Code+Stars)](https://github.com/mrjunjieli/ActiveExtract) [[Demo]](https://activeextract.github.io/) 

## Before
- Wang Honglong, Fu Yanjie,**<u>Li Junjie</u>**, et al. ‚ÄùStream Attention Based U-Net for L3DAS23
Challenge,‚Äù ICASSP 2023, pp. 1-2, doi: 10.1109/ICASSP49357.2023.10095854.
- **<u>Li Junjie</u>**, Ge Meng, et al. Rethinking the Visual Cues in Audio-Visual Speaker Extraction. Proc. INTER-
SPEECH 2023, 3754-3758, doi: 10.21437/Interspeech.2023-2545. [![](https://img.shields.io/github/stars/mrjunjieli/DAVSE?style=social&label=Code+Stars)](https://github.com/mrjunjieli/DAVSE)
- **<u>Li Junjie</u>**, Ge Meng, et al. ‚ÄùDeep Multi-task Cascaded Acoustic Echo Cancellation and Noise Suppres-
sion,‚Äù 2022 13th ISCSLP, pp. 130-134, doi: 10.1109/ISCSLP57327.2022.10037852.  [![](https://img.shields.io/github/stars/mrjunjieli/DMC_AEC?style=social&label=Code+Stars)](https://github.com/mrjunjieli/DMC_AEC)
- **<u>Li Junjie</u>**, Ge Meng, et al. VCSE: Time-Domain Visual-Contextual Speaker Extraction Network. Proc.
INTERSPEECH 2022, 906-910, doi: 10.21437/Interspeech.2022-11183. [![](https://img.shields.io/github/stars/mrjunjieli/LRS3_for_AVSS?style=social&label=Code+Stars)](https://github.com/mrjunjieli/LRS3_for_AVSS)
- **<u>Li Junjie</u>** and Liu Ding, ‚ÄúInformation bottleneck theory on convolutional neural networks,‚Äù Neural Pro-
cessing Letters, vol. 53, no. 2, pp. 1385‚Äì1400, 2021. (JCR Q3)  [![](https://img.shields.io/github/stars/mrjunjieli/IB_ON_CNN?style=social&label=Code+Stars)](https://github.com/mrjunjieli/IB_ON_CNN)

# üíª Open Source Toolkit
- WeSep [![](https://img.shields.io/github/stars/wenet-e2e/wesep?style=social&label=Code+Stars)](https://github.com/wenet-e2e/wesep)

# üéñ Honors and Awards
- 2016-2017 President‚Äôs Scholarship Second Class (top 7%) [[pdf]](../pdf/Award_Scholarship_2nd.pdf)
- 2016-2017 Merit Student (top 5%)[[pdf]](../pdf/Award_MeritStudent_.pdf)
- 2017-2018 President‚Äôs Scholarship Third Class (top 15%)[[pdf]](../pdf/Award_Scholarship_3nd_.pdf)
- 2018-2019 President‚Äôs Scholarship Third Class (top 15%)[[pdf]](../pdf/Award_Scholarship_3nd.pdf)
- 2018-2019 Merit Student (top 5%) [[pdf]](../pdf/Award_MeritStudent.pdf)
- 2020 Outstanding Graduate Award (top 5%) [[pdf]](../pdf/Award_Outstanding_Graduate_Award.pdf)
- 2021-2022 Honda Kiyoshi‚Äôs Advanced Speech Science Award [[pdf]](../pdf/hongda.pdf)

# üòÑ Academic Activities
- 2024.11. Ê∑±Âú≥Â§ßÊπæÂå∫Â≠¶ÊúØËÆ∫Âùõ [[Image]](../images/haizhou_birthday/index.html)
- 2024.08 [PolyU Research Student Conference](https://events.polyu.edu.hk/prsc2024/home) [[Image]](../images/PRSC/index.html)
- 2024.04 Attending ICASSP 2024, Korea. [[Image]](../images/icassp2024/index.html)
- 2024.03 ICASSP 2024 preview, organised by <a href='https://sds.cuhk.edu.cn/en/teacher/641'>Dr. Zhizheng WU </a>, Shenzhen. [[Image]](../images/icassp2024_preview/index.html)
- 2023.12 International Doctoral Forum 2023, CUHK. [[Image]](../images/CUHK2023/index.html)
- 2023.12 International Workshop on Mathematical Issues in Information Sciences 2023, CUHK(SZ). [[Image]](../images/MIIS2023/index.html)
- 2023.12 CHINA HI-TECH Forum 2023, Shenzhen. [[Image]](../images/HITECH2023/index.html)

# ‚úÖ Blog 
- [Summary of Speaker-realted tasks][![](https://img.shields.io/github/stars/mrjunjieli/awesome_speaker?style=social&label=Code+Stars)](https://github.com/mrjunjieli/awesome_speaker)

<!-- 
# üî• News
- *2022.02*: &nbsp;üéâüéâ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2022.02*: &nbsp;üéâüéâ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2016</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

**Kaiming He**, Xiangyu Zhang, Shaoqing Ren, Jian Sun

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>

- [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020**

# üéñ Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üìñ Educations
- *2019.06 - 2022.04 (now)*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üí¨ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# üíª Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China. -->
